{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "tkwargs = {\n",
    "    \"dtype\": torch.double,\n",
    "    \"device\": torch.device(\"cpu\") # \"cuda:0\" if torch.cuda.is_available() else \n",
    "}\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-bppxcu9p because the default path (/home/picarib/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "/home/picarib/anaconda3/lib/python3.8/site-packages/botorch/test_functions/base.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ref_point = torch.tensor(self._ref_point, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# from botorch.test_functions.multi_objective import DTLZ1, BraninCurrin\n",
    "\n",
    "\n",
    "# problem = DTLZ3(dim=100, negate=True).to(**tkwargs) # BraninCurrin(negate=True).to(**tkwargs) # \n",
    "\n",
    "from  problem.network_optimization.topology import NetTopology\n",
    "from problem.network_optimization.problem import *\n",
    "\n",
    "tempCacheInfo = {'Sydney1': {'size': 100, 'type': 'LRU'}, 'Brisbane2': {'size': 100, 'type': 'LRU'}, 'Canberra1': {'size': 100, 'type': 'LRU'}, \\\n",
    "                 'Sydney2': {'size': 100, 'type': 'LRU'}, 'Townsville': {'size': 100, 'type': 'LRU'}, 'Cairns': {'size': 100, 'type': 'LRU'}, \\\n",
    "                 'Brisbane1': {'size': 100, 'type': 'LRU'}, 'Rockhampton': {'size': 100, 'type': 'LRU'}, 'Armidale': {'size': 100, 'type': 'LRU'}, \\\n",
    "                 'Hobart': {'size': 100, 'type': 'LRU'}, 'Canberra2': {'size': 100, 'type': 'LRU'}, 'Perth1': {'size': 100, 'type': 'LRU'}, \\\n",
    "                 'Perth2': {'size': 100, 'type': 'LRU'}, 'Adelaide1': {'size': 100, 'type': 'LRU'}, 'Adelaide2': {'size': 100, 'type': 'LRU'}, \\\n",
    "                 'Melbourne1': {'size': 100, 'type': 'LRU'}, 'Melbourne2': {'size': 100, 'type': 'LRU'}, 'Alice Springs': {'size': 100, 'type': 'LRU'}, \\\n",
    "                 'Darwin': {'size': 100, 'type': 'LRU'}}\n",
    "bounds = [1, 100]\n",
    "topology = NetTopology('topology/Aarnet.gml', 'Sydney1', cacheDictInfo=tempCacheInfo)\n",
    "problem = CDNOptimizationProblem(topology, batch_size=BATCH_SIZE, runReqNums=1000, tkwargs=tkwargs, bounds=bounds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
    "\n",
    "NUM_RESTARTS = 20 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 1024 if not SMOKE_TEST else 4\n",
    "d = problem.dim\n",
    "standard_bounds = torch.zeros(2, problem.dim, **tkwargs)\n",
    "standard_bounds[1] = 1\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.fc21 = nn.Linear(50, 20)\n",
    "        self.fc22 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 50)\n",
    "        self.fc4 = nn.Linear(50, 100)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def decode(train_x):\n",
    "    with torch.no_grad():\n",
    "        decoded = vae_model.decode(train_x)\n",
    "    return decoded.view(train_x.shape[0], 1, 28, 28)\n",
    "\n",
    "def get_fitted_model(train_x, train_obj, state_dict=None):\n",
    "    # initialize and fit model\n",
    "    model = SingleTaskGP(train_X=train_x, train_Y=train_obj)\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    mll.to(train_x)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return model\n",
    "\n",
    "def optimize_qehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, problem.bounds)).mean\n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point=problem.ref_point, \n",
    "        Y=pred,\n",
    "    )\n",
    "    acq_func = qExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point,\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true\n",
    "\n",
    "def optimize_vae_qehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, problem.bounds)).mean\n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point=problem.ref_point, \n",
    "        Y=pred,\n",
    "    )\n",
    "    acq_func = qExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point,\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=torch.stack([\n",
    "            torch.zeros(d, **tkwargs), \n",
    "            torch.ones(d, **tkwargs),\n",
    "        ]),\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "    )\n",
    "\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true\n",
    "\n",
    "\n",
    "vae_model = VAE().to(**tkwargs)\n",
    "vae_state_dict = torch.load(\"vae.pt\", map_location=tkwargs['device'])\n",
    "vae_model.load_state_dict(vae_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization\n",
    "\n",
    "We use a list of `FixedNoiseGP`s to model the two objectives with known noise variances. Homoskedastic noise levels can be inferred by using `SingleTaskGP`s instead of `FixedNoiseGP`s.\n",
    "\n",
    "The models are initialized with $2(d+1)=6$ points drawn randomly from $[0,1]^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.gp_regression import FixedNoiseGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "\n",
    "NOISE_SE = torch.tensor([15.19, 0.63], **tkwargs)\n",
    "\n",
    "\n",
    "\n",
    "def generate_initial_data(n=6):\n",
    "    # generate training data\n",
    "    train_x = draw_sobol_samples(\n",
    "        bounds=problem.bounds,n=1, q=n, seed=torch.randint(1000000, (1,)).item()\n",
    "    ).squeeze(0).to(**tkwargs)\n",
    "    train_obj_true = problem(train_x)\n",
    "    train_obj = train_obj_true + torch.randn_like(train_obj_true) * NOISE_SE\n",
    "    return train_x, train_obj, train_obj_true\n",
    "\n",
    "\n",
    "def initialize_model(train_x, train_obj, state_dict=None):\n",
    "    # define models for objective and constraint\n",
    "    train_x = normalize(train_x, problem.bounds)\n",
    "    models = []\n",
    "    for i in range(train_obj.shape[-1]):\n",
    "        train_y = train_obj[..., i:i+1]\n",
    "        train_yvar = torch.full_like(train_y, NOISE_SE[i] ** 2)\n",
    "        models.append(\n",
    "            FixedNoiseGP(train_x, train_y, train_yvar, outcome_transform=Standardize(m=1))\n",
    "        )\n",
    "        \n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper functions that performs the essential BO step for $q$EHVI and $q$NEHVI\n",
    "The helper function below initializes the $q$EHVI acquisition function, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. \n",
    "\n",
    "For this example, we'll use a relatively small batch of optimization ($q=4$). For batch optimization ($q>1$), passing the keyword argument `sequential=True` to the function `optimize_acqf`specifies that candidates should be optimized in a sequential greedy fashion (see [1] for details why this is important). A simple initialization heuristic is used to select the 20 restart initial locations from a set of 1024 random points. Multi-start optimization of the acquisition function is performed using LBFGS-B with exact gradients computed via auto-differentiation.\n",
    "\n",
    "**Reference Point**\n",
    "\n",
    "$q$EHVI requires specifying a reference point, which is the lower bound on the objectives used for computing hypervolume. In this tutorial, we assume the reference point is known. In practice the reference point can be set 1) using domain knowledge to be slightly worse than the lower bound of objective values, where the lower bound is the minimum acceptable value of interest for each objective, or 2) using a dynamic reference point selection strategy.\n",
    "\n",
    "**Partitioning the Non-dominated Space into disjoint rectangles**\n",
    "\n",
    "$q$EHVI requires partitioning the non-dominated space into disjoint rectangles (see [1] for details). \n",
    "\n",
    "*Note:* `FastNondominatedPartitioning` *will be very slow when 1) there are a lot of points on the pareto frontier and 2) there are >5 objectives.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import FastNondominatedPartitioning\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement, qNoisyExpectedHypervolumeImprovement\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "\n",
    "NUM_RESTARTS = 20 if not SMOKE_TEST else 2\n",
    "RAW_SAMPLES = 1024 if not SMOKE_TEST else 4\n",
    "\n",
    "standard_bounds = torch.zeros(2, problem.dim, **tkwargs)\n",
    "standard_bounds[1] = 1\n",
    "\n",
    "\n",
    "def optimize_qehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, problem.bounds)).mean\n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point=problem.ref_point, \n",
    "        Y=pred,\n",
    "    )\n",
    "    acq_func = qExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point,\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrating over function values at in-sample designs**\n",
    "\n",
    "$q$NEHVI integrates over the unknown function values at the previously evaluated designs (see [2] for details). Therefore, we need to provide the previously evaluated designs (`train_x`, *normalized* to be within $[0,1]^d$) to the acquisition function.\n",
    "\n",
    "**Efficient batch generation with Cached Box Decomposition (CBD)**\n",
    "\n",
    "$q$NEHVI leveraged CBD to efficiently generate large batches of candidates. CBD scales polynomially with respect to the batch size where as the inclusion-exclusion principle used by qEHVI scales exponentially with the batch size.\n",
    "\n",
    "**Pruning baseline designs**\n",
    "To speed up integration over the function values at the previously evaluated designs, we prune the set of previously evaluated designs (by setting `prune_baseline=True`) to only include those which have positive probability of being on the current in-sample Pareto frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=problem.ref_point.tolist(),  # use known reference point \n",
    "        X_baseline=normalize(train_x, problem.bounds),\n",
    "        prune_baseline=True,  # prune baseline points that have estimated zero probability of being Pareto optimal\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that performs the essential BO step for $q$NParEGO\n",
    "The helper function below similarly initializes $q$NParEGO, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. \n",
    "\n",
    "$q$NParEGO uses random augmented chebyshev scalarization with the `qNoisyExpectedImprovement` acquisition function. In the parallel setting ($q>1$), each candidate is optimized in sequential greedy fashion using a different random scalarization (see [1] for details).\n",
    "\n",
    "To do this, we create a list of `qNoisyExpectedImprovement` acquisition functions, each with different random scalarization weights. The `optimize_acqf_list` method sequentially generates one candidate per acquisition function and conditions the next candidate (and acquisition function) on the previously selected pending candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
    "\n",
    "\n",
    "def optimize_qnparego_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Samples a set of random weights for each candidate in the batch, performs sequential greedy optimization \n",
    "    of the qNParEGO acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    train_x = normalize(train_x, problem.bounds)\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(train_x).mean\n",
    "    acq_func_list = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        weights = sample_simplex(problem.num_objectives, **tkwargs).squeeze()\n",
    "        objective = GenericMCObjective(get_chebyshev_scalarization(weights=weights, Y=pred))\n",
    "        acq_func = qNoisyExpectedImprovement(  # pyre-ignore: [28]\n",
    "            model=model,\n",
    "            objective=objective,\n",
    "            X_baseline=train_x,\n",
    "            sampler=sampler,\n",
    "            prune_baseline=True,\n",
    "        )\n",
    "        acq_func_list.append(acq_func)\n",
    "    # optimize\n",
    "    candidates, _ = optimize_acqf_list(\n",
    "        acq_function_list=acq_func_list,\n",
    "        bounds=standard_bounds,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "    )\n",
    "    # observe new values \n",
    "    new_x =  unnormalize(candidates.detach(), bounds=problem.bounds)\n",
    "    new_obj_true = problem(new_x)\n",
    "    new_obj = new_obj_true + torch.randn_like(new_obj_true) * NOISE_SE\n",
    "    return new_x, new_obj, new_obj_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Bayesian Optimization loop with $q$NEHVI, $q$EHVI, and $q$NParEGO\n",
    "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps:\n",
    "1. given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$\n",
    "2. observe $f(x)$ for each $x$ in the batch \n",
    "3. update the surrogate model. \n",
    "\n",
    "\n",
    "Just for illustration purposes, we run three trials each of which do `N_BATCH=30` rounds of optimization. The acquisition function is approximated using `MC_SAMPLES=128` samples.\n",
    "\n",
    "*Note*: Running this may take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial  1 of 3 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/picarib/anaconda3/lib/python3.8/site-packages/gpytorch/lazy/lazy_tensor.py:1741: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1672.)\n",
      "  Linv = torch.triangular_solve(Eye, L, upper=False).solution\n",
      "  3%|▎         | 1/30 [13:48<6:40:32, 828.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  1: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (446216.00, 446216.00, 446354.00, 446216.00, 446216.00), time = 828.70."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [23:03<5:11:26, 667.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  2: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (446216.00, 446216.00, 455120.00, 450833.00, 448646.00), time = 554.46."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [45:53<7:24:48, 988.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  3: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (446216.00, 448102.00, 455120.00, 453587.00, 448646.00), time = 1370.55."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [1:09:22<8:20:16, 1154.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  4: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (446216.00, 461288.00, 460145.00, 456298.00, 448646.00), time = 1408.94."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [1:25:05<7:29:14, 1078.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  5: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (446216.00, 461588.00, 460145.00, 456922.00, 566740.00), time = 942.96."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [1:34:59<6:05:20, 913.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  6: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (446216.00, 461588.00, 460810.00, 492063.00, 567622.00), time = 593.41."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [1:51:55<6:03:01, 947.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  7: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (446216.00, 462324.00, 461110.00, 492063.00, 569260.00), time = 1016.26."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [2:10:55<6:09:48, 1008.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  8: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (446216.00, 462324.00, 472160.00, 520156.00, 575920.00), time = 1140.43."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [2:30:44<6:12:44, 1064.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  9: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 474938.00, 528366.00, 520156.00, 580487.00), time = 1188.95."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [2:32:52<4:18:31, 775.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 10: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 494516.00, 528366.00, 524306.00, 580487.00), time = 127.57."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [2:35:35<3:06:18, 588.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 11: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 494516.00, 532110.00, 546913.00, 599303.00), time = 163.74."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [2:41:58<2:37:44, 525.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 12: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 494629.00, 551146.00, 557612.00, 599303.00), time = 382.84."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [2:45:35<2:02:27, 432.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 13: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 494629.00, 551146.00, 566187.00, 599303.00), time = 216.87."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [2:49:10<1:37:43, 366.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 14: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 494629.00, 559167.00, 571929.00, 599303.00), time = 214.60."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [2:52:27<1:18:51, 315.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 15: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 523820.00, 579555.00, 662024.00, 599303.00), time = 197.05."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [2:57:35<1:13:04, 313.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 16: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 523820.00, 579555.00, 662120.00, 610328.00), time = 307.91."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [3:05:25<1:18:05, 360.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 17: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 523820.00, 581191.00, 670236.00, 610328.00), time = 470.36."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [3:08:36<1:01:53, 309.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 18: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 523820.00, 610752.00, 675408.00, 686090.00), time = 190.92."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [3:16:00<1:04:08, 349.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 19: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 523820.00, 611859.00, 677228.00, 686090.00), time = 443.81."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [3:18:53<49:28, 296.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 20: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 523820.00, 613887.00, 684568.00, 686090.00), time = 173.42."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [3:20:57<36:44, 244.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 21: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 523820.00, 615327.00, 699126.00, 706366.00), time = 123.83."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [3:23:59<30:07, 225.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 22: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 622502.00, 615327.00, 699126.00, 706366.00), time = 181.80."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [3:27:36<26:03, 223.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 23: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 631052.00, 615327.00, 699126.00, 706366.00), time = 217.40."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [3:41:27<40:33, 405.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 24: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 631052.00, 615327.00, 700192.00, 706366.00), time = 830.48."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [3:43:41<27:00, 324.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 25: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 631052.00, 615327.00, 700192.00, 706366.00), time = 134.19."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [3:47:08<19:16, 289.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 26: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 631052.00, 615327.00, 700201.00, 706366.00), time = 207.05."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [3:49:13<11:59, 239.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 27: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 631052.00, 615327.00, 700483.00, 713630.00), time = 124.84."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [3:51:31<06:58, 209.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 28: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 631052.00, 617277.00, 702407.00, 713630.00), time = 138.56."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [3:55:47<03:43, 223.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 29: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 631052.00, 617477.00, 702407.00, 713630.00), time = 255.52."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [3:59:53<00:00, 479.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 30: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (450556.00, 631052.00, 617730.00, 702407.00, 740621.00), time = 246.20.\n",
      "Trial  2 of 3 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 1/30 [03:18<1:35:51, 198.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  1: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (239681.00, 239699.00, 239681.00, 239681.00, 239681.00), time = 198.32."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [09:42<2:23:41, 307.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  2: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (239681.00, 239699.00, 239681.00, 239681.00, 239967.00), time = 384.65."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [18:21<3:01:51, 404.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  3: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (243121.00, 242967.00, 239901.00, 239681.00, 239967.00), time = 518.61."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [25:04<2:54:55, 403.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  4: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (250932.00, 244263.00, 253897.00, 239681.00, 239967.00), time = 402.98."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [36:32<3:30:53, 506.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  5: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253227.00, 268529.00, 253897.00, 239681.00, 242586.00), time = 687.77."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [44:14<3:16:29, 491.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  6: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253227.00, 279638.00, 253897.00, 244590.00, 244706.00), time = 462.29."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [52:31<3:09:00, 493.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  7: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253227.00, 279638.00, 261053.00, 256626.00, 244706.00), time = 496.83."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [57:33<2:38:27, 432.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  8: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253227.00, 333953.00, 268715.00, 264771.00, 244706.00), time = 301.77."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [1:03:51<2:25:22, 415.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch  9: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253292.00, 333953.00, 268715.00, 264771.00, 386130.00), time = 378.42."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [1:15:32<2:47:51, 503.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 10: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253292.00, 333953.00, 285115.00, 267361.00, 389148.00), time = 701.13."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [1:18:57<2:10:28, 412.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 11: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253292.00, 333953.00, 292191.00, 285375.00, 389421.00), time = 204.50."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [1:28:05<2:16:02, 453.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 12: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253292.00, 371053.00, 292191.00, 306478.00, 389421.00), time = 548.24."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [1:36:15<2:11:39, 464.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 13: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253787.00, 373809.00, 308438.00, 394478.00, 389421.00), time = 490.38."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [1:41:50<1:53:24, 425.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 14: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253787.00, 373809.00, 308438.00, 397422.00, 416403.00), time = 334.19."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [1:44:35<1:26:41, 346.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 15: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253787.00, 384505.00, 312214.00, 397422.00, 416403.00), time = 164.95."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [1:46:48<1:05:54, 282.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 16: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253787.00, 384505.00, 312214.00, 397422.00, 416403.00), time = 133.12."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [1:50:28<57:08, 263.75s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 17: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (253787.00, 384505.00, 312214.00, 402994.00, 451858.00), time = 220.19."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [1:54:10<50:16, 251.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 18: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (263162.00, 384505.00, 333563.00, 426953.00, 453330.00), time = 222.60."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [1:57:55<44:35, 243.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 19: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (392587.00, 386089.00, 379682.00, 429585.00, 467371.00), time = 224.25."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [2:02:55<43:24, 260.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 20: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (393803.00, 386089.00, 532049.00, 429585.00, 467371.00), time = 300.67."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [2:08:43<42:59, 286.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 21: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (393803.00, 386089.00, 532049.00, 436659.00, 522054.00), time = 347.37."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [2:10:40<31:25, 235.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 22: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (393803.00, 523379.00, 655422.00, 436659.00, 524742.00), time = 116.89."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [2:12:39<23:26, 200.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 23: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (393803.00, 523405.00, 655422.00, 436659.00, 525303.00), time = 119.80."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [2:15:48<19:42, 197.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 24: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (393803.00, 523405.00, 655422.00, 444469.00, 539270.00), time = 188.49."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [2:19:11<16:34, 198.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 25: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (393803.00, 523405.00, 655422.00, 444469.00, 539270.00), time = 202.98."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [2:28:01<19:53, 298.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 26: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (396659.00, 523405.00, 655422.00, 445237.00, 540550.00), time = 530.34."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [2:31:58<13:59, 279.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 27: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (396659.00, 523405.00, 655422.00, 445985.00, 600444.00), time = 237.07."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [2:37:20<09:45, 292.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 28: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (396659.00, 523405.00, 655422.00, 447685.00, 600444.00), time = 321.88."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [2:45:40<05:54, 354.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 29: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = (396659.00, 523405.00, 656004.00, 549893.00, 600444.00), time = 499.61."
     ]
    }
   ],
   "source": [
    "from botorch import fit_gpytorch_model\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import DominatedPartitioning\n",
    "\n",
    "import time\n",
    "import warnings, tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "N_TRIALS = 3 if not SMOKE_TEST else 2\n",
    "N_BATCH = 30 if not SMOKE_TEST else 10\n",
    "MC_SAMPLES = 128  if not SMOKE_TEST else 16\n",
    "\n",
    "verbose = True\n",
    "\n",
    "hvs_qparego_all, hvs_qehvi_all, hvs_qnehvi_all, hvs_random_all, hvs_vae_qehvi_all = [], [], [], [], []\n",
    "\n",
    "\n",
    "# average over multiple trials\n",
    "for trial in range(1, N_TRIALS + 1):\n",
    "    torch.manual_seed(trial)\n",
    "    \n",
    "    print(f\"\\nTrial {trial:>2} of {N_TRIALS} \", end=\"\")\n",
    "    hvs_qparego, hvs_qehvi, hvs_qnehvi, hvs_random, hvs_vae_qehvi = [], [], [], [], []\n",
    "    \n",
    "    # call helper functions to generate initial training data and initialize model\n",
    "    train_x_qparego, train_obj_qparego, train_obj_true_qparego = generate_initial_data(n=2*(problem.dim+1))\n",
    "    mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "    \n",
    "    train_x_qehvi, train_obj_qehvi, train_obj_true_qehvi = train_x_qparego, train_obj_qparego, train_obj_true_qparego\n",
    "    train_x_qnehvi, train_obj_qnehvi, train_obj_true_qnehvi = train_x_qparego, train_obj_qparego, train_obj_true_qparego\n",
    "    train_x_vae_qehvi, train_obj_vae_qehvi, train_obj_true_vae_qehvi = train_x_qparego, train_obj_qparego, train_obj_true_qparego\n",
    "    train_x_random, train_obj_random, train_obj_true_random = train_x_qparego, train_obj_qparego, train_obj_true_qparego\n",
    "    mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
    "    mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi)\n",
    "    mll_vae_qehvi, model_vae_qehvi = initialize_model(train_x_vae_qehvi, train_obj_vae_qehvi)\n",
    "    # compute hypervolume\n",
    "    bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj_true_qparego)\n",
    "    volume = bd.compute_hypervolume().item()\n",
    "    \n",
    "    hvs_qparego.append(volume)\n",
    "    hvs_qehvi.append(volume)\n",
    "    hvs_qnehvi.append(volume)\n",
    "    hvs_random.append(volume)\n",
    "    hvs_vae_qehvi.append(volume)\n",
    "    state_dict = None\n",
    "    # run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "    for iteration in tqdm.tqdm(range(1, N_BATCH + 1)):    \n",
    "        \n",
    "        t0 = time.time()\n",
    "        # fit the models\n",
    "        fit_gpytorch_model(mll_qparego)\n",
    "        fit_gpytorch_model(mll_qehvi)\n",
    "        fit_gpytorch_model(mll_qnehvi)\n",
    "        fit_gpytorch_model(mll_vae_qehvi)\n",
    "        \n",
    "        # define the qEI and qNEI acquisition modules using a QMC sampler\n",
    "        qparego_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "        qehvi_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "        qnehvi_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "        vae_qehvi_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES)\n",
    "        \n",
    "        # optimize acquisition functions and get new observations\n",
    "        new_x_qparego, new_obj_qparego, new_obj_true_qparego = optimize_qnparego_and_get_observation(\n",
    "            model_qparego, train_x_qparego, train_obj_qparego, qparego_sampler\n",
    "        )\n",
    "        new_x_qehvi, new_obj_qehvi, new_obj_true_qehvi = optimize_qehvi_and_get_observation(\n",
    "            model_qehvi, train_x_qehvi, train_obj_qehvi, qehvi_sampler\n",
    "        )\n",
    "        new_x_qnehvi, new_obj_qnehvi, new_obj_true_qnehvi = optimize_qnehvi_and_get_observation(\n",
    "            model_qnehvi, train_x_qnehvi, train_obj_qnehvi, qnehvi_sampler\n",
    "        )\n",
    "        new_x_vae_qehvi, new_obj_vae_qehvi, new_obj_true_vae_qehvi = optimize_vae_qehvi_and_get_observation(\n",
    "            model_vae_qehvi, train_x_vae_qehvi, train_obj_vae_qehvi, vae_qehvi_sampler\n",
    "        )\n",
    "        new_x_random, new_obj_random, new_obj_true_random = generate_initial_data(n=BATCH_SIZE)\n",
    "                \n",
    "        # update training points\n",
    "        train_x_qparego = torch.cat([train_x_qparego, new_x_qparego])\n",
    "        train_obj_qparego = torch.cat([train_obj_qparego, new_obj_qparego])\n",
    "        train_obj_true_qparego = torch.cat([train_obj_true_qparego, new_obj_true_qparego])\n",
    "\n",
    "        train_x_qehvi = torch.cat([train_x_qehvi, new_x_qehvi])\n",
    "        train_obj_qehvi = torch.cat([train_obj_qehvi, new_obj_qehvi])\n",
    "        train_obj_true_qehvi = torch.cat([train_obj_true_qehvi, new_obj_true_qehvi])\n",
    "        \n",
    "        train_x_qnehvi = torch.cat([train_x_qnehvi, new_x_qnehvi])\n",
    "        train_obj_qnehvi = torch.cat([train_obj_qnehvi, new_obj_qnehvi])\n",
    "        train_obj_true_qnehvi = torch.cat([train_obj_true_qnehvi, new_obj_true_qnehvi])\n",
    "        \n",
    "        train_x_vae_qehvi = torch.cat([train_x_vae_qehvi, new_x_vae_qehvi])\n",
    "        train_obj_vae_qehvi = torch.cat([train_obj_vae_qehvi, new_obj_vae_qehvi])\n",
    "        train_obj_true_vae_qehvi = torch.cat([train_obj_true_vae_qehvi, new_obj_true_vae_qehvi])\n",
    "    \n",
    "        train_x_random = torch.cat([train_x_random, new_x_random])\n",
    "        train_obj_random = torch.cat([train_obj_random, new_obj_random])\n",
    "        train_obj_true_random = torch.cat([train_obj_true_random, new_obj_true_random])\n",
    "        \n",
    "        # update progress\n",
    "        for hvs_list, train_obj in zip(\n",
    "            (hvs_random, hvs_qparego, hvs_qehvi, hvs_qnehvi, hvs_vae_qehvi), \n",
    "            (train_obj_true_random, train_obj_true_qparego, train_obj_true_qehvi, train_obj_true_qnehvi, train_obj_true_vae_qehvi),\n",
    "        ):\n",
    "            # compute hypervolume\n",
    "            bd = DominatedPartitioning(ref_point=problem.ref_point, Y=train_obj)\n",
    "            volume = bd.compute_hypervolume().item()\n",
    "            hvs_list.append(volume)\n",
    "        \n",
    "        \n",
    "        state_dict = model_vae_qehvi.state_dict()\n",
    "        # reinitialize the models so they are ready for fitting on next iteration\n",
    "        # Note: we find improved performance from not warm starting the model hyperparameters\n",
    "        # using the hyperparameters from the previous iteration\n",
    "        mll_qparego, model_qparego = initialize_model(train_x_qparego, train_obj_qparego)\n",
    "        mll_qehvi, model_qehvi = initialize_model(train_x_qehvi, train_obj_qehvi)\n",
    "        mll_qnehvi, model_qnehvi = initialize_model(train_x_qnehvi, train_obj_qnehvi)\n",
    "        mll_vae_qehvi, model_vae_qehvi = initialize_model(train_x_vae_qehvi, train_obj_vae_qehvi)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"\\nBatch {iteration:>2}: Hypervolume (random, qNParEGO, qEHVI, qNEHVI, VAE_qEHVI) = \"\n",
    "                f\"({hvs_random[-1]:>4.2f}, {hvs_qparego[-1]:>4.2f}, {hvs_qehvi[-1]:>4.2f}, {hvs_qnehvi[-1]:>4.2f}, {hvs_vae_qehvi[-1]:>4.2f}), \"\n",
    "                f\"time = {t1-t0:>4.2f}.\", end=\"\"\n",
    "            )\n",
    "        else:\n",
    "            print(\".\", end=\"\")\n",
    "   \n",
    "    hvs_qparego_all.append(hvs_qparego)\n",
    "    hvs_qehvi_all.append(hvs_qehvi)\n",
    "    hvs_qnehvi_all.append(hvs_qnehvi)\n",
    "    hvs_random_all.append(hvs_random)\n",
    "    hvs_vae_qehvi_all.append(hvs_vae_qehvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def ci(y):\n",
    "    return 1.96 * y.std(axis=0) / np.sqrt(N_TRIALS)\n",
    "\n",
    "\n",
    "iters = np.arange(N_BATCH + 1) * BATCH_SIZE\n",
    "log_hv_difference_qparego = problem.max_hv - np.asarray(hvs_qparego_all) # np.log10(problem.max_hv - np.asarray(hvs_qparego_all))\n",
    "log_hv_difference_qehvi = problem.max_hv - np.asarray(hvs_qehvi_all) #np.log10(problem.max_hv - np.asarray(hvs_qehvi_all))\n",
    "log_hv_difference_qnehvi = problem.max_hv - np.asarray(hvs_qnehvi_all) #np.log10(problem.max_hv - np.asarray(hvs_qnehvi_all))\n",
    "log_hv_difference_rnd = problem.max_hv - np.asarray(hvs_random_all) #np.log10(problem.max_hv - np.asarray(hvs_random_all))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.errorbar(\n",
    "    iters, log_hv_difference_rnd.mean(axis=0), yerr=ci(log_hv_difference_rnd),\n",
    "    label=\"Sobol\", linewidth=1.5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    iters, log_hv_difference_qparego.mean(axis=0), yerr=ci(log_hv_difference_qparego),\n",
    "    label=\"qNParEGO\", linewidth=1.5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    iters, log_hv_difference_qehvi.mean(axis=0), yerr=ci(log_hv_difference_qehvi),\n",
    "    label=\"qEHVI\", linewidth=1.5,\n",
    ")\n",
    "ax.errorbar(\n",
    "    iters, log_hv_difference_qnehvi.mean(axis=0), yerr=ci(log_hv_difference_qnehvi),\n",
    "    label=\"qNEHVI\", linewidth=1.5,\n",
    ")\n",
    "ax.set(xlabel='number of observations (beyond initial points)', ylabel='Log Hypervolume Difference')\n",
    "ax.legend(loc=\"lower left\")\n",
    "print(problem.max_hv)\n",
    "print(hvs_qparego_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot the true objectives at the evaluated designs colored by iteration\n",
    "\n",
    "To examine optimization process from another perspective, we plot the true function values at the designs selected under each algorithm where the color corresponds to the BO iteration at which the point was collected. The plot on the right for $q$NEHVI shows that the $q$NEHVI quickly identifies the pareto front and most of its evaluations are very close to the pareto front. $q$NParEGO also identifies has many observations close to the pareto front, but relies on optimizing random scalarizations, which is a less principled way of optimizing the pareto front compared to $q$NEHVI, which explicitly attempts focuses on improving the pareto front. $q$EHVI uses the posterior mean as a plug-in estimator for the true function values at the in-sample points, whereas $q$NEHVI than integrating over the uncertainty at the in-sample designs Sobol generates random points and has few points close to the Pareto front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(23, 7), sharex=True, sharey=True)\n",
    "algos = [\"Sobol\", \"qNParEGO\", \"qEHVI\", \"qNEHVI\", \"VAE-qEHVI\"]\n",
    "cm = plt.cm.get_cmap('viridis')\n",
    "\n",
    "batch_number = torch.cat(\n",
    "    [torch.zeros(2*(problem.dim+1)), torch.arange(1, N_BATCH+1).repeat(BATCH_SIZE, 1).t().reshape(-1)]\n",
    ").numpy()\n",
    "for i, train_obj in enumerate((train_obj_true_random, train_obj_true_qparego, train_obj_true_qehvi, train_obj_true_qnehvi, train_obj_true_vae_qehvi)):\n",
    "    sc = axes[i].scatter(\n",
    "        train_obj[:, 0].cpu().numpy(), train_obj[:,1].cpu().numpy(), c=batch_number, alpha=0.8,\n",
    "    )\n",
    "    axes[i].set_title(algos[i])\n",
    "    axes[i].set_xlabel(\"Objective 1\")\n",
    "axes[0].set_ylabel(\"Objective 2\")\n",
    "norm = plt.Normalize(batch_number.min(), batch_number.max())\n",
    "sm =  ScalarMappable(norm=norm, cmap=cm)\n",
    "sm.set_array([])\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.93, 0.15, 0.01, 0.7])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "cbar.ax.set_title(\"Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
